# Gradient Descent Optimization Algorithms
## Introduction: 
 Save my gradient optimization algorithms code. <br>
 
 SG improved in three ways: <br>
 > Direction1-Noise Reduction Method <br>
 > Direction2-Variance Reduction Method (SG have residual, can't use fixed stepsize) <br>
 > Direction3-Second Order Method (escaping saddle point) <br>

## Algorithm listï¼š
>Basic-GradientDescentVariants:<br>
>> {Batch Gradient Descent } Algorithm: [BatchGradientDescent.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Basic-GradientDescentVariants/BatchGradientDescent.py) <br>
>> {Stochastic Gradient Descent } Algorithm: [StochasticGradientDescent.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Basic-GradientDescentVariants/StochasticGradientDescent.py) <br>
>> {Mini-Batch Gradient Descent } Algorithm: [Mini-Batch_GradientDescent.py](https://github.com/YEN-GitHub/StochasticGradient_Algorithm/tree/master/Basic-GradientDescentVariants/Mini-Batch_GradientDescent.py) <br>

> Direction1-Noise Reduction Method: <br>
>> { } Algorithm: [.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction1-NoiseReductionMethod/WolfeCondition.py) <br>

> Direction2-Variance Reduction Method:<br>
>> {  momentum stochastic gradient descent } Algorithm: [MomentumStochasticGradientDescent.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction2-VarianceReductionMethod/MomentumStochasticGradientDescent.py) <br>
>> { Nesterov Accelerated Gradient } Algorithm: [NesterovAcceleratedGradient.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction2-VarianceReductionMethod/NesterovAcceleratedGradient.py) <br>
>> { Adagrad } Algorithm: [Adagrad.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction2-VarianceReductionMethod/Adagrad.py) <br>
>> { Adadelta } Algorithm: [Adadelta.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction2-VarianceReductionMethod/Adadelta.py) <br>
>> { RMSprop } Algorithm: [RMSprop.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction2-VarianceReductionMethod/RMSprop.py) <br>
>> { Adam } Algorithm: [Adam.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction2-VarianceReductionMethod/Adam.py) <br>

> Direction3-Second Order Method:<br>
>> { } Algorithm: [.py](https://github.com/YEN-GitHub/GradientDescent_OptimizationAlgorithms/tree/master/Direction3-SecondOrderMethod/WolfeCondition.py) <br>

## References
> Paper:  `Optimization Methods for Large-Scale Machine Learning`  <br>
> Paper:  `An Overview of Gradient Descent Optimization Algorithms`  <br>
> Github user: `summersunshine1/optimize`   <br>
> Github user: `tsycnh/mlbasic`  <br>